{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b71070-d22f-46a4-afe7-6613fe1a1bd7",
   "metadata": {},
   "source": [
    "# Assignment 1: Evaluating Language Technologies\n",
    "This assignment must be completed in order to be considered for enrollment in the class. If you are already enrolled, failure to complete this assignment will results in your removal from the course.\n",
    "\n",
    "## Introduction\n",
    "In this assignment, we will explore different tasks core to the field of NLP. You will implement different evaluation metrics for these tasks and perform analysis of different language\n",
    "technologies.\n",
    "\n",
    "Learning objectives:\n",
    "* Understand the breadth of tasks core to the field of NLP.\n",
    "* Implement various evaluation methods for core NLP tasks.\n",
    "* Become familiar with prompting existing LLMs via API calls.\n",
    "* Identify the limitations of automatic evaluation methods.\n",
    "* Practice using error analysis to identify where language technologies fail and hypothesizing why they might fail.\n",
    "* Appreciate things we humans do with language that current language models cannot do.\n",
    "\n",
    "**Notes:**\n",
    "* In your solution, keep all code as-is except where it's explicitly mentioned to implement a function.\n",
    "* Items marked with a star (★) should be answered in markdown text in the notebook. You will include your final notebook as a pdf with your submission.\n",
    "* We will automatically save files with results to the results sub-directory, these will be used for autograding. You should submit the results directory with your assignment on gradescope.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "You will submit:\n",
    "\n",
    "* A PDF copy of your completed notebook. This should be titled \"HW1.pdf\". This is used to grade your answers to the problems marked with a (★).\n",
    "* All the json files saved inside the `results/` directory. These will be autograded.\n",
    "\n",
    "**Setup:**\n",
    "You will need to download ollama https://ollama.com and install the following models with the commands:\n",
    "\n",
    "`ollama pull gpt-oss:20b`\n",
    "\n",
    "`ollama pull llama3.2:1b`\n",
    "\n",
    "`ollama pull llama3.2`\n",
    "\n",
    "`ollama pull gemma3:1b`\n",
    "\n",
    "`ollama pull gemma3:4b`\n",
    "\n",
    "You will need about 30Gb of storage to download these models. Make sure the ollama binary is in /usr/local/bin/ or you know where it is, so you can call it on the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6b394-b22f-4066-b190-dd5aab1909a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbeeff-564a-4244-884e-a8ee46404219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "async def run_inference(model_name, prompt, temperature=0.0, max_tokens=16):\n",
    "    \"\"\"\n",
    "    curl http://localhost:11434/api/generate -d '{\n",
    "\t\t\"model\": \"llama3.2\",\n",
    "\t\t\"prompt\":\"Why is the sky blue?\",\n",
    "\t\t\"stream\": false,\n",
    "\t}'\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"temperature\": temperature,\n",
    "                    \"num_predict\": max_tokens,\n",
    "                },\n",
    "            }\n",
    "        ) as response:\n",
    "            result = await response.json()\n",
    "            return result.get(\"response\", \"\")\n",
    "\n",
    "async def run_batch_inference(model_name, prompts, temperature=0.0, max_tokens=16):\n",
    "    progress = tqdm(total=len(prompts))\n",
    "    async def _run_inference(prompt):\n",
    "        result = await run_inference(model_name, prompt, temperature, max_tokens)\n",
    "        progress.update(1)\n",
    "        return result\n",
    "    tasks = [_run_inference(prompt) for prompt in prompts]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "def cache_inferences(model_name, exp_name, prompts, inferences):\n",
    "    if os.path.exists(f\"inference_cache/{model_name}_{exp_name}.json\"):\n",
    "        with open(f\"inference_cache/{model_name}_{exp_name}.json\", \"r\") as f:\n",
    "            cache = json.load(f)\n",
    "            cache.update(dict(zip(prompts, inferences)))\n",
    "    else:\n",
    "        cache = dict(zip(prompts, inferences))\n",
    "    with open(f\"inference_cache/{model_name}_{exp_name}.json\", \"w\") as f:\n",
    "        json.dump(cache, f)\n",
    "\n",
    "def load_inferences(model_name, exp_name, prompts):\n",
    "    if os.path.exists(f\"inference_cache/{model_name}_{exp_name}.json\"):\n",
    "        with open(f\"inference_cache/{model_name}_{exp_name}.json\", \"r\") as f:\n",
    "            cache = json.load(f)\n",
    "            return [cache[prompt] if prompt in cache else None for prompt in prompts]\n",
    "    return [None for _ in prompts]\n",
    "\n",
    "def get_remaining_prompts(model_name, exp_name, prompts):\n",
    "    if os.path.exists(f\"inference_cache/{model_name}_{exp_name}.json\"):\n",
    "        with open(f\"inference_cache/{model_name}_{exp_name}.json\", \"r\") as f:\n",
    "            cache = json.load(f)\n",
    "            return [prompt for prompt in prompts if prompt not in cache]\n",
    "    return prompts\n",
    "\n",
    "models = [\n",
    "    \"llama3.2:1b\" ,\n",
    "    \"llama3.2\",\n",
    "    \"gemma3:1b\",\n",
    "    \"gemma3:4b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715a350-767a-4b35-907c-dacc830b4edc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 1: Text classification (25 points)\n",
    "Here, we will evaluate different LLMs on their ability to perform newsgroup classification. You will implement four standard metrics for evaluating a classifier and create a visualization of the errors using a confusion matrix. We will be using a classic NLP dataset, 20 Newsgroups (http://qwone.com/~jason/20Newsgroups/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54633538-2174-4c91-be97-657c25413db6",
   "metadata": {},
   "source": [
    "### Part 1.0: Setup\n",
    "Load the data we will be using for evaluation. This is the development set of the dataset, so it's ok for us to do analysis with it.\n",
    "\n",
    "We will load the possible labels from the training set, rather than the development set. If a new label has to be predicted at test-time, we wouldn't have learned it from training only on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bca4dd-802d-4092-8a83-c4547d19a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_inputs = pd.read_csv('A1_Data/dev_data.csv')\n",
    "classification_labels = pd.read_csv('A1_Data/dev_labels.csv')\n",
    "classification_data = pd.merge(classification_inputs, classification_labels, on=\"id\", how=\"inner\")[:200]\n",
    "\n",
    "print(f\"Loaded newsgroup data of shape {classification_data.shape} with columns {classification_data.columns.tolist()}.\")\n",
    "\n",
    "labels = pd.read_csv('A1_Data/train_labels.csv')[\"newsgroup\"].unique()\n",
    "label_list = \", \".join(sorted(list(labels)))\n",
    "\n",
    "print(f\"Found {len(labels)} possible target labels: \")\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca60294-1796-4eef-8258-7683ebfba3a9",
   "metadata": {},
   "source": [
    "Now let's run inference with the different classifiers. It will take some time to run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264e752-3963-40ff-ad72-fe3d1396c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newsgroup_prompt(text):\n",
    "\tnewsgroup_prompt = \"In which of the following newsgroups is the following email most likely to appear? Output only the newsgroup name, no other text.\"\n",
    "\tnewsgroup_prompt += f\"\\n\\nNewsgroups:\\n{label_list}\\n\\nEmail:\\n{text}\"\n",
    "\treturn newsgroup_prompt\n",
    "\n",
    "prompts = [get_newsgroup_prompt(text) for text in classification_data[\"text\"]]\n",
    "for model in models:\n",
    "\tprint(f\"Running inference for {model}\")\n",
    "\tremaining_prompts = get_remaining_prompts(model, \"pt1_0_classification_data\", prompts)\n",
    "\tcurr_inferences = await run_batch_inference(model, remaining_prompts, temperature=0.0, max_tokens=16)\n",
    "\tcache_inferences(model, \"pt1_0_classification_data\", remaining_prompts, curr_inferences)\n",
    "\tinferences = load_inferences(model, \"pt1_0_classification_data\", prompts)\n",
    "\tclassification_data[f\"predictions_{model}\"] = inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acfaaf-10c4-444d-87e8-9a3517f0f860",
   "metadata": {},
   "source": [
    "### Part 1.1: Confusion Matrix (5 points)\n",
    "Complete the following functions. The first one should computes a confusion matrix for a set of predictions, and the second one should use your favorite tool (e.g. Matplotlib) to visualize it as a heatmap. \n",
    "\n",
    "We're implementing the confusion matrix first because you might find it useful when completing the rest of Part 1.\n",
    "\n",
    "If the model messes up the output format (e.g. outputs something that isn't one of the labels), the prediction should not be included in the matrix counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fe25c-4433-462a-9a92-bab45eaea113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1_1_cm(x: DataFrame, y: str) -> Dict[str, Dict[str, int]]:\n",
    "    pass\n",
    "\n",
    "def part_1_1_vis(x: Dict[str, Dict[str, int]]):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66d61f-0f7c-4328-8d6f-d34fc9369dde",
   "metadata": {},
   "source": [
    "Now we'll use these functions to compute and visualize confusion matrices for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27026d26-2ac7-447b-8151-223c3d2122cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices = dict()\n",
    "\n",
    "for model in models:\n",
    "    confusion_matrices[model] = part1_1_cm(classification_data, model)\n",
    "    part_1_1_vis(confusion_matrices[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed7aaf-07e9-48fd-8b36-c41f0a144d31",
   "metadata": {},
   "source": [
    "★ Make sure the images are rendered in the notebook pdf you submit.\n",
    "\n",
    "★ For each of the different classifiers, which pair of classes is most often confused?\n",
    "\n",
    "★ Use the scratch space below to explore the documents that are misclassified most often with these class pairings, and include snippets of a few examples below.\n",
    "\n",
    "★ Why might these pairs of classes be more difficult for the classifiers to distinguish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e026d34-bf19-4486-83f5-276332eb9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use this scratch space to find documents \n",
    "# that are examples of common misclassfications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db2e04d-5c97-49b4-bcf6-740cba40dd52",
   "metadata": {},
   "source": [
    "### Part 1.2: Accuracy (5 points)\n",
    "Complete the following function, then use it to compute and compare the accuracies of each classifier. \n",
    "\n",
    "★ Generate a table comparing the accuracy of each of the classifier systems.\n",
    "\n",
    "★ Which one has the highest accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51293767-6c11-42d6-b081-a8e7e52606c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1_2_acc(x: DataFrame, y: str) -> float:\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "accuracies: dict[str, float] = dict()\n",
    "\n",
    "for model in models:\n",
    "    accuracies[model] = part1_2_acc(classification_data, model)\n",
    "\n",
    "save_results(accuracies, \"results/accuracies.json\")\n",
    "\n",
    "best_model = \"\" # TODO: Which model performed the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f489d-a357-46ca-9cfc-2e23b760ac81",
   "metadata": {},
   "source": [
    "### Part 1.3: Precision (5 points)\n",
    "Complete the following function that returns, for each class, the precision of the classifier's labels given ground-truth labels. (note: if there are no predictions for a given class, then precision should be 0)\n",
    "\n",
    "★ Generate a table showing, for the best-performing classifier, the precision of its labels across each class.\n",
    "\n",
    "★ For which class are its labels the most accurate? The least?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa74c72-48f6-4e87-94b5-6d366ea04b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1_3_prec(x: DataFrame, y: str) -> dict[str, float]:\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "best_model_prec = part1_3_prec(classification_data, best_model)\n",
    "\n",
    "save_results(best_model_prec, \"results/best_model_prec.json\")\n",
    "\n",
    "# TODO: Scratch space for analyzing the precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661f601-719d-40a3-8cce-a85be4c58e11",
   "metadata": {},
   "source": [
    "### Part 1.4: Recall (5 points)\n",
    "Complete the following function that returns, for each target class, the recall of the classifier's labels. (note: if there are no ground truth labels for a given class, the recall should be 0)\n",
    "\n",
    "★ Generate a table showing, for each target class, the recall of the best-performing model's labels across each class.\n",
    "\n",
    "★ For which class is recall the highest? The lowest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced6998-6fb7-4939-864d-773e08edef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1_4_rec(x: DataFrame, y: str) -> dict[str, float]:\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "best_model_rec = part1_4_rec(classification_data, best_model)\n",
    "\n",
    "save_results(best_model_rec, \"results/best_model_rec.json\")\n",
    "\n",
    "# TODO: Scratch space for analyzing the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84323e-af15-4b61-a3e8-9f7a7e9909bc",
   "metadata": {},
   "source": [
    "### Part 1.5: F1 (5 points)\n",
    "\n",
    "A weighted average F1 score computes an average F1 score over all target classes, weighted by the true number of instances of that class:\n",
    "\n",
    "![test](A1_Data/f1.png)\n",
    "\n",
    "Here, *f* is a classification function, *C* is the set of classes, *D* is the set of evaluation data with ground-truth labels, and *D_c* is the set of data whose ground-truth label is *c*.\n",
    "\n",
    "Implement two functions below:\n",
    "* A function that returns, for a target class, the F1 of a classifier's labels given ground truth labels\n",
    "* A function that uses the previous function to compute a weighted average F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735ea62-76cc-4474-af51-5317bd45a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1_5_class_f1(x: DataFrame, y: str, z: str) -> float:\n",
    "    # TODO: Implement this function\n",
    "    # note: z is the class we want to compute F1 for, and y is the model name string\n",
    "    pass\n",
    "\n",
    "def part1_5_weighted_f1(x: DataFrame, y: str) -> float:\n",
    "    # TODO: Implement this function\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d919f-e4c9-4dca-bb0c-aaa5d649a5a7",
   "metadata": {},
   "source": [
    "★ Generate a table comparing the weighted F1 score of each classifier.\n",
    "\n",
    "★ Which is the best classifier according to weighted F1? How and why might this differ from an accuracy comparison (Part 1.2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ae9df-79b5-4800-86eb-0e6bac50b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s: dict[str, float] = dict()\n",
    "\n",
    "for model in models:\n",
    "    f1s[model] = part1_5_weighted_f1(classification_data, model)\n",
    "\n",
    "save_results(f1s, \"results/f1s.json\")\n",
    "\n",
    "# TODO: Scratch space for analyzing and reporting the F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b39a2-ac8e-4a3d-bd7f-9ecdcdbf57bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 2: Automatic speech recognition (25 points)\n",
    "Here, we'll explore using a popular open-source automatic speech recognition system to compare its performance across English speakers whose native language differs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f531156-b41d-4dbd-ba8c-3697a47b57a3",
   "metadata": {},
   "source": [
    "### Part 2.0: Setup\n",
    "First we'll download the Speech Accent Archive dataset (https://www.kaggle.com/datasets/rtatman/speech-accent-archive/data) -- it's about a 1GB download and may take some time to download and extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1be48e-e7b2-4eaf-b6fb-f020100d0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "speech_accent_path = kagglehub.dataset_download(\"rtatman/speech-accent-archive\")\n",
    "speakers = pd.read_csv(f\"{speech_accent_path}/speakers_all.csv\")\n",
    "language_counts = speakers['native_language'].value_counts()\n",
    "\n",
    "languages_with_over_20_samples = set(language_counts[language_counts >= 20].index)\n",
    "print(languages_with_over_20_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27064a-5a8b-4415-aa41-4daf3bf0ee41",
   "metadata": {},
   "source": [
    "Now let's load and run WhisperX to get predictions for each of the relevant audio files. This might take some time to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522fb025-0ab7-417e-8b4f-eca58fb3c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "whisperx_model = whisperx.load_model(\"tiny\", \"cpu\", compute_type=\"float32\", language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f62d69-ae67-4e37-8323-1c56f8e768bd",
   "metadata": {},
   "source": [
    "Now let's run inference. This is the smallest model we could run inference on, so it's performance isn't going to be great. But, it shouldn't take *too* long to run on your local machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf9960-f48a-488f-be38-0d6731825bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "transcriptions = {language: list() for language in languages_with_over_20_samples}\n",
    "\n",
    "need_to_transcribe = False\n",
    "\n",
    "if os.path.exists('transcriptions.tsv'):\n",
    "    with open('transcriptions.tsv') as infile:\n",
    "        for line in infile.readlines():\n",
    "            language, transcript = line.strip().split(\"\\t\")\n",
    "            transcriptions[language].append(transcript)\n",
    "\n",
    "        for language, transcripts in transcriptions.items():\n",
    "            if len(transcripts) != 20:\n",
    "                need_to_transcribe = True\n",
    "else:\n",
    "    need_to_transcribe = True\n",
    "\n",
    "if need_to_transcribe:\n",
    "    recordings_filepath = f\"{speech_accent_path}/recordings/recordings/\"\n",
    "    for language in languages_with_over_20_samples:\n",
    "        print(f\"Transcribing for native language {language}...\")\n",
    "        for i in tqdm(range(20)):\n",
    "            filename = f\"{language}{i + 1}.mp3\"\n",
    "            audio = whisperx.load_audio(f\"{recordings_filepath}/{filename}\")\n",
    "            transcription = whisperx_model.transcribe(audio, language=\"en\")\n",
    "            transcriptions[language].append(transcription)       \n",
    "            \n",
    "    with open(\"transcriptions.tsv\", \"w\") as ofile: \n",
    "        for language, transcripts in transcriptions.items():\n",
    "            for transcript in transcripts:\n",
    "                text = \" \".join([segment[\"text\"] for segment in transcript[\"segments\"]])\n",
    "                ofile.write(f\"{language}\\t{text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41657dd-2bae-4f86-a923-59a25959b899",
   "metadata": {},
   "source": [
    "### Part 2.1: Word error rate (20 points)\n",
    "Implement the following function to compute the word error rate for a candidate transcription given a reference. Each of the recordings we transcribed are readings of the same English text: *\"Please call Stella.  Ask her to bring these things with her from the store:  Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids.  She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555d757-f2cf-4045-b51a-8283b0eda039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def part2_1(x: list[str], y: list[str]) -> float:\n",
    "    # TODO: Implement this function.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702de923-e641-460f-bbcb-6be921981c90",
   "metadata": {},
   "source": [
    "Then let's evaluate for all of the candidate transcriptions and each of the native languages.\n",
    "\n",
    "★ Generate a table comparing the average WER across each of the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9e07a-2d7a-496a-a1c7-f378bc1ac00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "reference = \"Please call Stella.  Ask her to bring these things with her from the store: \" \\\n",
    "            \"Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack \" \\\n",
    "            \"for her brother Bob.  We also need a small plastic snake and a big toy frog for the kids. \" \\\n",
    "            \"She can scoop these things into three red bags, and we will go meet her Wednesday at the \"\\\n",
    "            \"train station.\"\n",
    "\n",
    "language_wers = {language: list() for language in languages_with_over_20_samples}\n",
    "for language, transcripts in transcriptions.items():\n",
    "    for candidate in transcripts:\n",
    "        language_wers[language].append(part2_1(nltk.word_tokenize(reference), nltk.word_tokenize(candidate)))\n",
    "\n",
    "save_results(language_wers, \"results/language_wers.json\")\n",
    "\n",
    "# TODO: Scratch space for computing average WER and generating a table to compare across native languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df1f83a-2cc2-4d48-a2e8-5b594adc2f7c",
   "metadata": {},
   "source": [
    "### Part 2.2: Error analysis (5 points)\n",
    "★ Which languages have the lowest WER? Which have the highest?\n",
    "\n",
    "★ Why might these languages have lower WER?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e0cbb-5308-4fee-a7b9-e76919b4f957",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 3: Machine translation (25 points)\n",
    "In this part we will evaluate models on their abilities to translate text via BLEU score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647419ed-d7a9-4409-9901-7b9716f55b70",
   "metadata": {},
   "source": [
    "### Part 3.0: Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00a7d9",
   "metadata": {},
   "source": [
    "Prepare the dataset we will use for evaluation. This is a standard czech to english translation dataset. We will just use the first 25 examples for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88843107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset_cs = load_dataset(\"wmt/wmt19\", \"cs-en\", split=\"validation\")\n",
    "hf_dataset_zh = load_dataset(\"wmt/wmt19\", \"zh-en\", split=\"validation\")\n",
    "hf_dataset_gu = load_dataset(\"wmt/wmt19\", \"gu-en\", split=\"validation\")\n",
    "\n",
    "dataset_items_cs = []\n",
    "for i, item in enumerate(list(hf_dataset_cs)[:25]):\n",
    "    dataset_items_cs.append({\n",
    "        'id': i,\n",
    "        'czech_text': item['translation']['cs'],\n",
    "        'english_text': item['translation']['en']\n",
    "    })\n",
    "\n",
    "dataset_items_zh = []\n",
    "for i, item in enumerate(list(hf_dataset_zh)[:25]):\n",
    "    dataset_items_zh.append({\n",
    "        'id': i,\n",
    "        'chinese_text': item['translation']['zh'],\n",
    "        'english_text': item['translation']['en']\n",
    "    })\n",
    "\n",
    "dataset_items_gu = []\n",
    "for i, item in enumerate(list(hf_dataset_gu)[:25]):\n",
    "    dataset_items_gu.append({\n",
    "        'id': i,\n",
    "        'gujarati_text': item['translation']['gu'],\n",
    "        'english_text': item['translation']['en']\n",
    "    })\n",
    "\n",
    "dataset_cs = pd.DataFrame(dataset_items_cs)\n",
    "dataset_zh = pd.DataFrame(dataset_items_zh)\n",
    "dataset_gu = pd.DataFrame(dataset_items_gu)\n",
    "print(f\"Loaded translation data of shape {dataset_cs.shape} with columns {dataset_cs.columns.tolist()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7bfe3",
   "metadata": {},
   "source": [
    "We will use a simple nltk based tokenizer to encode the n-gram tokens for computing the bleu score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Download required NLTK data if not already downloaded\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    # Tokenize the text into a list of tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f906de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text(\"Welcome to CS-183: we are learning about NLP evaluations today!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac17459",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2187fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2e363",
   "metadata": {},
   "source": [
    "now let's get the model predictions for these translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62832bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_prompt(text):\n",
    "\ttranslation_prompt = \"Translate the following text from Czech to English. Output only the translation, no other text.\"\n",
    "\ttranslation_prompt += f\"\\n\\nText:\\n{text}\"\n",
    "\treturn translation_prompt\n",
    "\n",
    "prompts_cs = [get_translation_prompt(text) for text in dataset_cs[\"czech_text\"]]\n",
    "for model in models:\n",
    "\tprint(f\"Running inference for {model}\")\n",
    "\tremaining_prompts = get_remaining_prompts(model, \"pt3_1_translation_data_cs\", prompts_cs)\n",
    "\tcurr_inferences = await run_batch_inference(model, remaining_prompts, temperature=0.0, max_tokens=512)\n",
    "\tcache_inferences(model, \"pt3_1_translation_data_cs\", remaining_prompts, curr_inferences)\n",
    "\tinferences = load_inferences(model, \"pt3_1_translation_data_cs\", prompts_cs)\n",
    "\tdataset_cs[f\"predictions_{model}\"] = inferences\n",
    "\n",
    "prompts_zh = [get_translation_prompt(text) for text in dataset_zh[\"chinese_text\"]]\n",
    "for model in models:\n",
    "\tprint(f\"Running inference for {model}\")\n",
    "\tremaining_prompts = get_remaining_prompts(model, \"pt3_1_translation_data_zh\", prompts_zh)\n",
    "\tprint(f\"Remaining prompts: {len(remaining_prompts)}\")\n",
    "\tcurr_inferences = await run_batch_inference(model, remaining_prompts, temperature=0.0, max_tokens=512)\n",
    "\tcache_inferences(model, \"pt3_1_translation_data_zh\", remaining_prompts, curr_inferences)\n",
    "\tinferences = load_inferences(model, \"pt3_1_translation_data_zh\", prompts_zh)\n",
    "\tdataset_zh[f\"predictions_{model}\"] = inferences\n",
    "\n",
    "prompts_gu = [get_translation_prompt(text) for text in dataset_gu[\"gujarati_text\"]]\n",
    "for model in models:\n",
    "\tprint(f\"Running inference for {model}\")\n",
    "\tremaining_prompts = get_remaining_prompts(model, \"pt3_1_translation_data_gu\", prompts_gu)\n",
    "\tcurr_inferences = await run_batch_inference(model, remaining_prompts, temperature=0.0, max_tokens=512)\n",
    "\tcache_inferences(model, \"pt3_1_translation_data_gu\", remaining_prompts, curr_inferences)\n",
    "\tinferences = load_inferences(model, \"pt3_1_translation_data_gu\", prompts_gu)\n",
    "\tdataset_gu[f\"predictions_{model}\"] = inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c699755",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e04d37",
   "metadata": {},
   "source": [
    "### Part 3.1 n-gram precision (5 points)\n",
    "\n",
    "complete the following function to calculate the n-gram prediction of a predicted translation given the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ng_prec(pred, ref, n):\n",
    "    # TODO: Implement this function.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b4780b",
   "metadata": {},
   "source": [
    "### Part 3.2 brevity penalty (5 points)\n",
    "\n",
    "complete the following function to calculate the brevity penalty for the bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f973a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brev_pen(pred, ref):\n",
    "    # TODO: Implement this function.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee6eef",
   "metadata": {},
   "source": [
    "### Part 3.3 putting it together (10 points)\n",
    "\n",
    "complete the following function to calculate the full bleu score, using the two functions from the previous parts. Represent the bleu score as discussed in class, using the 1, 2, 3, and 4 gram precisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a5ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(pred, ref):\n",
    "    # TODO: Implement this function.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505efcc2",
   "metadata": {},
   "source": [
    "★ Generate a table computing the bleu score for all model outputs for each language pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores_cs = {model: [] for model in models}\n",
    "bleu_scores_zh = {model: [] for model in models}\n",
    "bleu_scores_gu = {model: [] for model in models}\n",
    "\n",
    "references_cs = dataset_cs['english_text']\n",
    "references_zh = dataset_zh['english_text']\n",
    "references_gu = dataset_gu['english_text']\n",
    "\n",
    "for model in models:\n",
    "    predictions_cs = dataset_cs[f'predictions_{model}']\n",
    "    for reference, prediction in zip(references_cs, predictions_cs):\n",
    "        bleu_scores_cs[model].append(bleu(prediction, reference))\n",
    "    \n",
    "    predictions_zh = dataset_zh[f'predictions_{model}']\n",
    "    for reference, prediction in zip(references_zh, predictions_zh):\n",
    "        bleu_scores_zh[model].append(bleu(prediction, reference))\n",
    "    \n",
    "    predictions_gu = dataset_gu[f'predictions_{model}']\n",
    "    for reference, prediction in zip(references_gu, predictions_gu):\n",
    "        bleu_scores_gu[model].append(bleu(prediction, reference))\n",
    "\n",
    "save_results(bleu_scores_cs, \"results/bleu_scores_cs.json\")\n",
    "save_results(bleu_scores_zh, \"results/bleu_scores_zh.json\")\n",
    "save_results(bleu_scores_gu, \"results/bleu_scores_gu.json\")\n",
    "\n",
    "# TODO: Scratch space for computing average BLEU and generating a table to compare across models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d767fc",
   "metadata": {},
   "source": [
    "### Part 3.4 Error Analysis (5 points)\n",
    "\n",
    "★ Look at these and other lowest-scoring documents. Identify 3 types of errors this MT system seems to be making and show two example documents per error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a53c9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part 4: Open-ended text generation (25 points)\n",
    "\n",
    "In this part we will evaluate open ended text using an LLM judge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa064b4",
   "metadata": {},
   "source": [
    "### Part 4.0: Setup\n",
    "\n",
    "Here we list the prompts we will use for comparing LLMs. Since the judge should ideally be more powerful than the models generating the outputs, we will use the GPT-oss 20B model as the judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\t\"Explain bleu score in 100 words or less.\",\n",
    "    \"Why does bleu score use a brevity penalty?\",\n",
    "    \"How is precision different from recall?\",\n",
    "    \"Why would one want to use F1-score instead of accuracy?\",\n",
    "    \"What are the best boba spots in Berkeley?\",\n",
    "    \"Why would an LLM judge prefer one output over another?\",\n",
    "    \"What does CS 61C at UC Berkeley teach?\",\n",
    "    \"What do most students at UC Berkeley think of EE16a?\",\n",
    "    \"What is the best place to get ramen in Berkeley?\",\n",
    "    \"Write a poem about Berkeley.\",\n",
    "]\n",
    "\n",
    "data = pd.DataFrame(prompts, columns=[\"prompt\"])\n",
    "\n",
    "judge_model = \"gpt-oss:20b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf2eea",
   "metadata": {},
   "source": [
    "Get the outputs from each of the models for these prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_per_model = {}\n",
    "for model in models:\n",
    "\tprint(f\"Running inference for {model}\")\n",
    "\tremaining_prompts = get_remaining_prompts(model, \"pt3_1_pairwise_judgement_data\", prompts)\n",
    "\tcurr_inferences = await run_batch_inference(model, remaining_prompts, temperature=0.0, max_tokens=1024*8)\n",
    "\tcache_inferences(model, \"pt3_1_pairwise_judgement_data\", remaining_prompts, curr_inferences)\n",
    "\tinferences = load_inferences(model, \"pt3_1_pairwise_judgement_data\", prompts)\n",
    "\toutputs_per_model[model] = inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ecbaf",
   "metadata": {},
   "source": [
    "Have a look at some of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use this scratch space to explore the model outputs\n",
    "outputs_per_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db064257",
   "metadata": {},
   "source": [
    "### Part 4.1 Pairwise judge (10 Points)\n",
    "\n",
    "Let's implement a function that takes as input a prompt and two candidate generated texts and calls an LLM to judge which of the two texts is preferred.\n",
    "\n",
    "We've implemented the prompt for the judge below. You should compute the head to head comparisons for all models across all prompt pairs. Include the pairwise judgement for each pair/prompt in the head_to_head_judgements object below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa09cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_judgement_prompt(output1, output2):\n",
    "\tprompt = f\"Which of the following two outputs is better? Output 1: {output1}. Output 2: {output2}. Output only the number 1 or 2, no other text.\"\n",
    "\treturn prompt\n",
    "\n",
    "head_to_head_judgements = [{str((model_a, model_b)): None for model_a in models for model_b in models if model_a != model_b} for _ in range(len(prompts))]\n",
    "\n",
    "# TODO: impelemnt the head to head judgements\n",
    "\n",
    "save_results(head_to_head_judgements, \"results/head_to_head_judgements.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df0fc4",
   "metadata": {},
   "source": [
    "★ Which model performs best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471ef48",
   "metadata": {},
   "source": [
    "### Part 4.2: Attribute Evaluation (5 Points)\n",
    "\n",
    "Instead of judging outputs in a pairwise fashion, let's instead implement a judge which takes a prompt and outputs a score on a scale of 1-5 for the following attributes:\n",
    "\n",
    "* Style - is the output well written and entertaining, without being excessive or over the top in its style?\n",
    "* Correctness - does the output contain only factually correct information?\n",
    "* Brevity - is the output short and to the point and does not contain unnecessary slop?\n",
    "\n",
    "This time you will write the prompt. Include your attrbite predictions in the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e93b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_preds = [[{'style': None, 'correctness': None, 'brevity': None} for _ in models] for _ in prompts]\n",
    "\n",
    "# TODO: Implement the attribute predictions\n",
    "\n",
    "save_results(attribute_preds, \"results/attribute_preds.json\")\n",
    "\n",
    "# TODO: Scratch space for computing average attribute scores and generating a table to compare across models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dab51b",
   "metadata": {},
   "source": [
    "★ Which models win on each attribute?\n",
    "\n",
    "★ How do these results correlate with the head-to-head comparitons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d74134",
   "metadata": {},
   "source": [
    "### Part 4.3: Human Evaluation and Error Analysis (5 Points)\n",
    "\n",
    "★ How do your preferences and ratings correlate or don’t correlate with the LLM-as-a-judge labels?\n",
    "\n",
    "★ What is one additional attribute you might want to evaluate via LLM-as-a-judge so that the evaluation better captures your preferences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143ba11",
   "metadata": {},
   "source": [
    "### Part 4.4: Prompt and Model Sensitivity (5 Points)\n",
    "\n",
    "Re-run the head-to-head LLM-as-a-judge evaluation in Part 4.1, but instead re-phrase the prompt.\n",
    "\n",
    "Include your outputs in the new_head_to_head_judgements object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ec615",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_head_to_head_judgements = [{str((model_a, model_b)): None for model_a in models for model_b in models if model_a != model_b} for _ in range(len(prompts))]\n",
    "\n",
    "# TODO: Implement the new head to head judgements\n",
    "\n",
    "save_results(new_head_to_head_judgements, \"results/new_head_to_head_judgements.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d7209",
   "metadata": {},
   "source": [
    "★ How does the evaluation change when you change the prompt?\n",
    "\n",
    "★ How might you adjust your evaluation methodology to account for this variation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e7254",
   "metadata": {},
   "source": [
    "## Part 5: Frontier of Model Capabilities (optional; 10 bonus points)\n",
    "\n",
    "In this optional part you will invent and evaluate a new task that you believe modern LLMs might struggle with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec2e6e",
   "metadata": {},
   "source": [
    "### Part 5.1: Invent a new task (5 points)\n",
    "\n",
    "Brainstorm a task that you think modern models might not be able to do (yet, or ever).\n",
    "\n",
    "Report:\n",
    "\n",
    "★ A natural language description of the task.\n",
    "\n",
    "★ A few instances of the task, showing inputs and expected outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd860977",
   "metadata": {},
   "source": [
    "### Part 5.1: Evaluate a model on this task (5 points)\n",
    "\n",
    "How well do current models perform on this task?\n",
    "\n",
    "Report:\n",
    "\n",
    "★ Explain how you might evaluate this task. What challenges might arise in evaluation?\n",
    "\n",
    "★ Try evaluating a frontier model (e.g. LLM, VLM) on this task. Which model did you\n",
    "use? How did it perform? Show examples of its outputs. If it performed poorly, what do you think are the main challenges involved in task success that the model can’t capture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9e569",
   "metadata": {},
   "source": [
    "## Homework Feedback (optional)\n",
    "\n",
    "★ This is a new assignment so we are open to feedback. What was good/bad about this assignment? What should we have done differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9cc6cc",
   "metadata": {},
   "source": [
    "## Academic integrity\n",
    "\n",
    "You can discuss high-level solutions with classmates, but your submitted work should be your own. Don't copy from or share code with one another. Provide attribution for any sources of assistance you used in the assignment. If you use generative AI tools to aid in any parts of this assignment, describe how you used them and what worked (or didn't work) when using them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs183_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
